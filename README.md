# ğŸ¤ Interview Helper AI

**Interview Helper AI** â€“ your personal real-time co-pilot for interviews, meetings, and technical discussions.  
It listens, analyzes, and helps you answer confidently â€” all while staying private and running locally or with cloud-powered AI.

---

## ğŸ¯ Overview

Interview Helper AI assists you in **answering interview questions naturally and confidently**.  
It transcribes and interprets questions from **audio, text, or screenshots**, then generates **ready-to-speak, first-person answers** â€” so you sound like the best version of yourself.

---

## ğŸ§  Powered by Gemini & Ollama

- **Gemini (Google AI)** â€“ fast, accurate cloud intelligence  
- **Ollama (Local AI)** â€“ fully private, offline processing  

Choose between cloud or local AI for maximum control and flexibility.

---

## ğŸš€ Quick Start

### Prerequisites
- Node.js (v18+)
- Git
- Either:
  - **Google Gemini API key** (from [Google AI Studio](https://makersuite.google.com/app/apikey))
  - **or** [Ollama](https://ollama.ai) installed locally for private AI use

---

### Installation

1. **Clone the repository**
   ```bash
   git clone [repository-url]
   cd interview-helper-ai
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

   If you encounter Sharp or Python errors:
   ```bash
   SHARP_IGNORE_GLOBAL_LIBVIPS=1 npm install --ignore-scripts
   npm rebuild sharp
   ```

3. **Create a `.env` file** in the root directory and add your configuration:

   **For Gemini (Cloud AI):**
   ```env
   GEMINI_API_KEY=your_api_key_here
   ```

   **For Ollama (Local AI):**
   ```env
   USE_OLLAMA=true
   OLLAMA_MODEL=llama3.2
   OLLAMA_URL=http://localhost:11434
   ```

---

### Running the App

#### Development Mode
```bash
npm start
```
- Launches the Vite dev server on port 5173  
- Starts the Electron desktop app  
- Connects automatically to Gemini or Ollama  

#### Production Build
```bash
npm run dist
```
The compiled app will be available in the `release` folder.

---

## âš™ï¸ AI Providers

### ğŸ§© Ollama (Local AI â€“ Recommended for Privacy)

**Pros**
- 100% local, no data leaves your machine  
- Free and works offline  
- Supports models like `llama3.2`, `mistral`, `codellama`, and others  

**Setup**
```bash
brew install ollama   # or download from https://ollama.ai
ollama pull llama3.2
ollama serve
```

### â˜ï¸ Google Gemini

**Pros**
- Advanced reasoning and multimodal understanding  
- Fastest responses and highest accuracy  

**Cons**
- Requires API key  
- Internet access required  
- Usage may incur costs  

---

## ğŸ–¥ï¸ Usage Tips

### Stopping the App
- Press `Ctrl + C` (Windows/Linux) or `Cmd + C` (Mac) in the terminal  
- If ports remain busy:
  ```bash
  lsof -i :5173
  kill -9 [PID]
  ```
- Or run:
  ```bash
  npm run clean
  ```

### Keyboard Shortcuts

| Shortcut | Action |
|-----------|---------|
| `Cmd/Ctrl + B` | Toggle window visibility |
| `Cmd/Ctrl + H` | Capture screenshot |
| `Cmd/Ctrl + Enter` | Generate AI answer |
| `Cmd/Ctrl + Arrows` | Move the window |

---

## ğŸ§© Core Features

### ğŸ¤ Real-Time Interview Assistant
- Converts interviewerâ€™s audio into text  
- Generates clear, confident first-person answers  
- Sounds natural and human-like â€” as if **you** are speaking  

### ğŸ–¼ï¸ Visual Understanding
- Analyze screenshots or code snippets  
- Get contextual explanations or direct answers  

### ğŸ’¬ Contextual Conversation
- Ask follow-ups naturally  
- Keeps track of the conversation topic  

### ğŸ›¡ï¸ Privacy Options
- Use **Ollama** for 100% offline mode  
- Screenshots auto-deleted after processing  
- No data tracking or remote logging  

### ğŸ§  Smart Answer Generation
- Custom-tuned prompts for interview scenarios  
- Tailored for QA, Dev, and Software Engineering domains  
- Fluent, natural, and ready-to-speak phrasing  

---

## ğŸ§‘â€ğŸ’» Use Cases

### ğŸ§¾ Interview Coaching
```
âœ“ Practice technical or behavioral interviews  
âœ“ Generate realistic, first-person answers  
âœ“ Simulate HR or coding challenges  
âœ“ Improve confidence and fluency
```

### ğŸ§© Technical Support
```
âœ“ Debug code snippets or screenshots  
âœ“ Explain test automation concepts  
âœ“ Simplify QA methodologies
```

### ğŸ™ï¸ Meeting or Presentation Assistant
```
âœ“ Summarize discussions  
âœ“ Suggest key talking points  
âœ“ Generate concise spoken responses
```

---

## âš ï¸ Troubleshooting

### Common Issues

**Port Conflict**
```bash
kill -9 $(lsof -t -i:5173)
```

**Sharp/Python Errors**
```bash
SHARP_IGNORE_GLOBAL_LIBVIPS=1 npm install --ignore-scripts
npm rebuild sharp
```

**App Not Launching**
- Ensure Vite is using port `5173`  
- Confirm Ollama is running (`ollama serve`)  

---

## ğŸ§© Supported Models

- **Gemini 2.0 Flash** â€“ Cloud AI with vision and reasoning  
- **Llama 3.2** â€“ Local, fast, high-quality text generation  
- **Mistral** â€“ Lightweight and responsive  
- **CodeLlama** â€“ Specialized for coding tasks  
- **Custom Models** â€“ Any model supported by Ollama  

---

## ğŸ–¥ï¸ System Requirements

```
Minimum:  4GB RAM, Dual-core CPU
Recommended: 8GB RAM, Quad-core CPU
Optimal:   16GB+ RAM for local AI models
```

---

## ğŸ¤ Contributing

Contributions are welcome!  
You can help improve Interview Helper AI by adding:

- New AI model integrations  
- Enhanced prompts for interview types  
- UI/UX improvements  
- Documentation and translations  

Submit a pull request or open an issue on GitHub.

---

## ğŸ“„ License

**ISC License** â€“ Free for personal and commercial use.

---

## â­ Support

If this project helps you prepare for interviews or practice your communication skills, please â­ star the repo on GitHub!

---

### ğŸ·ï¸ Tags
`interview-ai` `voice-assistant` `qa-engineer` `ollama` `gemini-ai` `electron` `vite` `cross-platform` `ai-coach` `privacy-first` `local-ai` `audio-analysis` `real-time-assistant` `technical-interview`